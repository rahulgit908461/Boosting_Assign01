{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83790bcf-e8fc-4358-abc2-7d76ad604717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1682754-039c-4513-ac52-02d14faf1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting is a method used in machine learning to reduce errors in predictive data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84eead5-63cc-4f84-bd81-abb51fa2388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98819bc2-865f-4f49-9746-218d2051c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The key benefits of boosting include:\n",
    "#Ease of Implementation: Boosting can be used with several hyper-parameter tuning options to improve fitting. ...\n",
    "#Reduction of bias: Boosting algorithms combine multiple weak learners in a sequential method, iteratively \n",
    "#improving upon observations.\n",
    "#Limitation :Complex: It is complex to handle all models' working and increase the data's weight from every \n",
    "#error. Algorithms are complicated to run in real time.\n",
    "#Dependency: Each successor model is dependent on the last model which may result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff82c96-b652-4bf1-a298-2530e0f2c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65635fdd-ed19-4ee7-a39b-312a82e7df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize \n",
    "#training errors. In boosting, a random sample of data is selected, fitted with a model and then trained \n",
    "#sequentiallyâ€”that is, each model tries to compensate for the weaknesses of its predecessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb7b0f7-be5f-460c-b0da-75876efd4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "153a362b-b81c-4542-86ec-a4055f2adf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Types of Boodting Algotithms\n",
    "#AdaBoost (Adaptive Boosting) algorithm.\n",
    "#Gradient Boosting algorithm.\n",
    "#XG Boost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd6d8d63-0ec8-443d-b12f-93707b4e40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f1d342-7d16-4b67-a5fc-c18fc8ab060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting uses three main tuning parameters: the number of trees, a shrinkage parameter and the number of splits \n",
    "#per tree. Cross-validation should be used to decide on the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc70539b-d969-45fa-b802-b9476bbe67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b2b5c67-afc2-49aa-8ae4-350bb9bd2e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basically boosting is to train weak learners sequentially, each trying to correct its predecessor. \n",
    "#For boosting, we need to specify a weak model (e.g. regression, shallow decision trees, etc.), and then we try \n",
    "#to improve each weak learner to learn something from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84a08391-128f-4b41-9711-ae85df1543ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f70d450f-8293-43e8-a8c1-1e5626839078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost, also called Adaptive Boosting, is a technique in Machine Learning used as an Ensemble Method.\n",
    "#The most common estimator used with AdaBoost is decision trees with one level which means Decision trees \n",
    "#with only 1 split. These trees are also called Decision Stumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0480bd0c-40ca-4f25-98d0-419f8973774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35b3fcb0-cc5f-43df-af1f-6ac05a57a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case of Adaptive Boosting or AdaBoost, it minimises the exponential loss function that can make the algorithm\n",
    "#sensitive to the outliers. With Gradient Boosting, any differentiable loss function can be utilised. Gradient \n",
    "#Boosting algorithm is more robust to outliers than AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d1a0ac9-208f-411d-9736-8f29bfeb6541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efb64ce3-cfb8-4564-84e3-ca1784e6dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After training a classifier at any level, AdaBoost assigns weight to each training item. Misclassified item is\n",
    "#assigned a higher weight so that it appears in the training subset of the next classifier with a higher\n",
    "#probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33d060d8-748b-496a-aedb-9c7c75377692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b5badf-6198-47a8-bd64-6393838fbe28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
